{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osgeo\n",
    "import os\n",
    "from os.path import join as pjoin\n",
    "import rioxarray\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "#import matplotlib as plt\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import geopandas as gpd\n",
    "from rasterio import features\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import r2_score,mean_squared_error\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "scratch_dir = './data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the 2009 classifications folder\n",
    "aoi = gpd.read_file(\"data/Train/classifications/Classifications.shp\")\n",
    "aoi = aoi[['layer', 'is_glacier', 'geometry']]\n",
    "\n",
    "# change the 0 to a 2 because later in the process all 0's will be removed\n",
    "aoi['is_glacier'].loc[aoi['is_glacier']==0] = 2\n",
    "print (aoi.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training data\n",
    "file_location = 'data/train_data_resampledNDSI.tif'\n",
    "d = rioxarray.open_rasterio(file_location).squeeze() \n",
    "d.coords['band'] = d.coords['band'] +2008\n",
    "d  # Show data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also prepare the color codes for visualization\n",
    "colors = [(257, 71, 27), (98, 93, 78)]\n",
    "cols = []\n",
    "for col in colors:\n",
    "    pal = sns.light_palette(col, input=\"husl\", n_colors=4)\n",
    "    for rgb in pal[1:]:\n",
    "        cols.append(rgb)\n",
    "# Assign color codes to LULC types \n",
    "symbology = {'Glacier': cols[2],\n",
    "             'Non Glacier': cols[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels from the AOIs\n",
    "aoi['aoi_cat'] = pd.Categorical(aoi['is_glacier'])\n",
    "\n",
    "# Rasterize\n",
    "rst = rasterio.open('data/train_data_resampledNDSI.tif')  # Base image to rasterize the *.shp\n",
    "meta = rst.meta.copy()  # Copy metadata from the base image\n",
    "meta.update(compress='lzw')\n",
    "\n",
    "# Burn the AOIs *.shp file into raster and save it\n",
    "out_rst = 'data/train/aoi_rasterizedNDSI.tif'\n",
    "with rasterio.open(out_rst, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # Create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(aoi.geometry, aoi.aoi_cat))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "    out.write_band(1, burned)\n",
    "\n",
    "stack = np.array([])\n",
    "\n",
    "print(d.shape)  # Each time, also check the size of the image\n",
    "    \n",
    "# In order to do clustering, image should be reshaped into a single column\n",
    "band_col = d.values.reshape(-1, 1)\n",
    "   \n",
    "# Each time put the reshaped image into the stack\n",
    "stack = np.hstack((stack, band_col)) if stack.size else band_col\n",
    "# Also to check the size of the stack\n",
    "print(stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the validation classes\n",
    "aoi2 = gpd.read_file(\"data/Val/classifications/valclass1999.shp\")\n",
    "aoi2 = aoi2[['layer', 'is_glacier', 'geometry']]\n",
    "\n",
    "# change the 0 to a 2 because later in the process all 0's will be removed\n",
    "aoi2['is_glacier'].loc[aoi2['is_glacier']==0] = 2\n",
    "print (aoi2.tail())\n",
    "\n",
    "# get the validation data\n",
    "file_location2 = 'data/validation_data_resampledNDSI.tif'\n",
    "d2 = rioxarray.open_rasterio(file_location2).squeeze() \n",
    "d2.coords['band'] = d2.coords['band'] +1998\n",
    "\n",
    "# Labels from the AOIs\n",
    "aoi2['aoi_cat'] = pd.Categorical(aoi2['is_glacier'])\n",
    "\n",
    "# Rasterize\n",
    "rst2 = rasterio.open('data/validation_data_resampledNDSI.tif')  # Base image to rasterize the *.shp\n",
    "meta = rst2.meta.copy()  # Copy metadata from the base image\n",
    "meta.update(compress='lzw')\n",
    "\n",
    "# Burn the AOIs *.shp file into raster and save it\n",
    "out_rst = 'data/val/aoi_rasterizedNDSI.tif'\n",
    "with rasterio.open(out_rst, 'w+', **meta) as out:\n",
    "    out_arr = out.read(1)\n",
    "\n",
    "    # Create a generator of geom, value pairs to use in rasterizing\n",
    "    shapes = ((geom,value) for geom, value in zip(aoi.geometry, aoi.aoi_cat))\n",
    "\n",
    "    burned = features.rasterize(shapes=shapes, fill=0, out=out_arr, transform=out.transform)\n",
    "    out.write_band(1, burned)\n",
    "\n",
    "stack2 = np.array([])\n",
    "\n",
    "print(d2.shape)  # Each time, also check the size of the image\n",
    "    \n",
    "# In order to do clustering, image should be reshaped into a single column\n",
    "band_col2 = d2.values.reshape(-1, 1)\n",
    "   \n",
    "# Each time put the reshaped image into the stack\n",
    "stack2 = np.hstack((stack2, band_col2)) if stack2.size else band_col2\n",
    "# Also to check the size of the stack\n",
    "print(stack2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the rasterized LULC types in the AOI and concatenate it together with the images\n",
    "\n",
    "aoi_rst = rasterio.open('data/train/aoi_rasterizedNDSI.tif').read(1)\n",
    "aoi_rst2 = rasterio.open('data/val/aoi_rasterizedNDSI.tif').read(1)\n",
    "\n",
    "# Stack the label with the input bands\n",
    "data = np.c_[stack, aoi_rst.reshape(-1,)]\n",
    "data2 = np.c_[stack2, aoi_rst2.reshape(-1,)]\n",
    "\n",
    "# Of course, we are only interested in pixels with LULC type labelled\n",
    "data = data[np.where(data[:,data.shape[1]-1]!=0)]\n",
    "data2 = data2[np.where(data2[:,data2.shape[1]-1]!=0)]\n",
    "\n",
    "\n",
    "X_train = data[:, :1] \n",
    "Y_train = data[:, 1:].reshape(-1,) \n",
    "\n",
    "X_test = data2[:, :1] \n",
    "Y_test = data2[:, 1:].reshape(-1,)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the classes\n",
    "aoi['is_glacier'].loc[aoi['is_glacier'] == 1] = 'Glacier'\n",
    "aoi['is_glacier'].loc[aoi['is_glacier'] == 2] = 'Non Glacier'\n",
    "aoi2['is_glacier'].loc[aoi2['is_glacier'] == 1] = 'Glacier'\n",
    "aoi2['is_glacier'].loc[aoi2['is_glacier'] == 2] = 'Non Glacier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree object\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {'max_depth': [2, 3, 4, 5, 6, 8, 10],\n",
    "              'min_samples_split': [2,3, 4,5, 6, 8, 10],\n",
    "              'min_samples_leaf': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(dt, param_grid, cv=5)\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a tree model\n",
    "tree_depth = grid_search.best_params_['max_depth']\n",
    "min_sample_split= grid_search.best_params_['min_samples_split'] \n",
    "min_samples_leaf= grid_search.best_params_['min_samples_leaf'] \n",
    "model_tree = DecisionTreeClassifier(max_depth=tree_depth, min_samples_split=min_sample_split, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "# Fit the model to your data.\n",
    "# Please note that the output of this fitting is a model with several parameters that are configurable, so far you only configured \"max_depth\" while training/fitting.\n",
    "model_tree.fit(X_train, Y_train)\n",
    "\n",
    "# Visualize split\n",
    "fig = plt.figure(figsize=(tree_depth*4,tree_depth*3))\n",
    "tree.plot_tree(model_tree, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = pd.Series(list(model_tree.predict(X_test[:,0].reshape(-1,1))), name='DT prediction')  # Store the predicted value in Y_pred\n",
    "Y_actu = pd.Series(list(Y_test), name='Manual delineation')\n",
    "\n",
    "# Map the LULC codes to the actual name of LULC types\n",
    "\n",
    "# First we need a mapping from the LULC codes to the actual LULC type name.\n",
    "code_lulc = { 1: 'Glacier',\n",
    "              2: 'Non Glacier'}\n",
    "\n",
    "# Now replace the non-intuitive numbers with actual LULC type names and store them into new variables\n",
    "Y_actu2 = Y_actu.replace(code_lulc)\n",
    "Y_pred2 = Y_pred.replace(code_lulc)\n",
    "\n",
    "# Show the LULC coded confusion matrix\n",
    "df_confusion2 = pd.crosstab(Y_actu2, Y_pred2)\n",
    "df_confusion2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out more detailed accuracy assessment report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(Y_actu2, Y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_all = model_tree.predict(stack2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig1,(ax1, ax2) = plt.subplots(1,2, figsize=(20,10))\n",
    "d2.plot(ax=ax1, cmap='gray', alpha=0.25, add_colorbar=False)  # Plot the satellite image\n",
    "aoi2.plot(ax=ax1, column='land_cover', legend=True, color=aoi2['is_glacier'].map(symbology))\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "custom_points = [Line2D([0], [0], marker=\"o\", linestyle=\"none\", markersize=5, color=color) for color in symbology.values()]\n",
    "leg_points = ax1.legend(custom_points, symbology.keys(), loc='upper right', frameon=False)\n",
    "ax1.add_artist(leg_points)\n",
    "\n",
    "# Assign color codes to LULC types \n",
    "symbology2 = {1: cols[2],\n",
    "              2: cols[0]}\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "cm = ListedColormap(symbology2.values())\n",
    "imin = min(symbology2)  # Colormap range\n",
    "imax = max(symbology2)\n",
    "\n",
    "# Visualize\n",
    "# Because the predicted labels are still in one column, you need to reshape it back to original image shape\n",
    "row, col = d2.shape  # Get the original dimensions of the image\n",
    "imin = min(symbology2)  # Colormap range\n",
    "imax = max(symbology2)\n",
    "\n",
    "print('Printing large image takes time...')\n",
    "ax2.imshow(Y_pred_all.reshape(row, col), cmap=cm, interpolation='none', vmin=imin, vmax=imax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the deployment data\n",
    "file_location3 = 'data/resampledNDSI.tif'\n",
    "data_all = rioxarray.open_rasterio(file_location3).squeeze() \n",
    "data_all.coords['band'] = data_all.coords['band'] +1991\n",
    "print(data_all.shape)\n",
    "\n",
    "# remove the artificialy created na's during the resampling\n",
    "data_all = data_all.dropna(dim = 'band', how = 'all')\n",
    "\n",
    "\n",
    "stack_all = np.array([])\n",
    "\n",
    "print(data_all.shape)  # Each time, also check the size of the image\n",
    "    \n",
    "# In order to do clustering, image should be reshaped into a single column\n",
    "band_col_all = data_all.values.reshape(-1, 1)\n",
    "   \n",
    "# Each time put the reshaped image into the stack\n",
    "stack_all = np.hstack((stack_all, band_col_all)) if stack_all.size else band_col_all\n",
    "# Also to check the size of the stack\n",
    "print(stack_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check is there is missing data, impute this with iterative imputer\n",
    "np.isnan(stack_all).sum()\n",
    "imp = IterativeImputer(max_iter = 10, random_state=42)\n",
    "stack_all = imp.fit_transform(stack_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploy model\n",
    "Y_pred_all = model_tree.predict(stack_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "results = xr.DataArray(Y_pred_all.reshape(data_all.shape),\n",
    "                       coords ={'time':data_all.coords['band'], \n",
    "                                'y': data_all.coords['y'], \n",
    "                                'x': data_all.coords['x']}, \n",
    "                                dims=['band', 'y', 'x'])\n",
    "results.rio.to_raster(pjoin(scratch_dir, 'resultsNDSI.tif'), compress='LZW')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
